[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics with Python",
    "section": "",
    "text": "Selected topics on statistics using scipy, scikit-learn and numpyro. This ebook is formatted using quarto"
  },
  {
    "objectID": "chapters/modeling/part1.html",
    "href": "chapters/modeling/part1.html",
    "title": "2  Parametric Modeling",
    "section": "",
    "text": "Modeling is a key step in the pipeline of statistical inference. In this series of lectures we will learn how to fit parametric and non-parametric models to our data.\n\nIn the parametric case we will focus on the classical frequentist method to fit models: maximum likelihood estimation (MLE)\nThen we will review non-parametric modeling techniques such as the Histogram and Kernel density estimation\nAfter that we will go back to parametric modeling and extend MLE using priors: Maximum a posteriori\nFinally we will present key ideas on bayesian modeling that are to be further developed in a future series of lectures\n\n\n\nInference is:\n\nTo draw conclusions from facts through a scientific premise\n\nIn the particular case of Statistical inference we have\n\nFacts: Observed data\nPremise: Probabilistic model\nConclusion: An unobserved quantity of interest\nObjective: Quantify the uncertainty of the conclusion given the data and the premise\n\nThe following are examples examples of statistical inference tasks:\n\nParameter estimation: What is the best estimate of a model parameter based on the observed data?\nConfidence estimation: How trustworthy is our point estimate?\nHypothesis testing: Is the data consistent with a given hypothesis or model?\n\n\n\n\nTo conduct inference we start by defining a statistical model\nModels can be broadly classified as:\nParametric:\n\nIt corresponds to an analytical function (distribution) with free parameters\nHas an a-priori fixed number of parameters\nIn general: Stronger assumptions, easier to interpret, faster to use\n\nNon-parametric:\n\nDistribution-free model but they do have parameters and assumptions (e.g. dependence)\nThe number of parameters depends on the amount of training data\nIn general: More flexible, harder to train\n\n\n\n\nThere are two main paradigms or perspectives for statistical inference: Frequentist (F) or classical and Bayesian (B). There are conceptual differences between these paradigms, for example\nDefinition of probability:\n\nF: Relative frequency of an event. An objective property of the real world\nB: Degree of subjective belief. Probability statements can be made not only on data but also on parameters and models themselves\n\nInterpretation of parameters:\n\nF: They are unknown and fixed constants\nB: They have distributions that quantify the uncertainty of our knowledge about them. We can compute expected values of the parameters\n\nIn this lecture we will focus on the frequentist approach to modeling. We will review the bayesian perspective in a future lesson"
  },
  {
    "objectID": "chapters/modeling/part1.html#frequentist-approach-on-parametric-modeling",
    "href": "chapters/modeling/part1.html#frequentist-approach-on-parametric-modeling",
    "title": "2  Parametric Modeling",
    "section": "2.2 Frequentist approach on parametric modeling",
    "text": "2.2 Frequentist approach on parametric modeling\nIn the case of parametric inference we assume that observations follow a certain distribution, i.e. observations are a realization of a random process (sampling)\nThe conceptual (iterative) steps of parametric inference are:\n\nModel fitting: Find parameters by fitting data to the current model\nModel proposition: Propose a new model that accommodates important features of the data better than the previous one\n\nIn the frequentist approach step 1 is typically solved using Maximum Likelihood Estimation (MLE). Other frequentist alternatives are the Method of Moments (MoM) and the M-estimator. Only MLE is covered in this lecture\n\n2.2.1 The likelihood\nThe likelihood function is a quantitative description of our experiment (measuring process), and it is the starting point for parametric modeling for both the frequentist and bayesian paradigms. In simple terms the likelihood tells us how good our model is with respect to the observed data\nLet’s now give a mathematical description of the likelihood\n\nSuppose we have an experiment that we model as a set of R.Vs \\(X_1, X_2, \\ldots, X_N\\)\nWe also have observations/realizations from our R.Vs \\(\\{x_i\\} = x_1, x_2, \\ldots, x_N\\)\nWe assume that the R.Vs follow a certain joint probability distribution \\(f(x_1, x_2, \\ldots, x_N | \\theta)\\) with parameters \\(\\theta\\)\n\nThe likelihood function is then defined as\n\\[\n\\begin{align}\n\\mathcal{L}(\\theta) &= P(X_1=x_1, X_2=x_2, \\ldots, X_N=x_n) \\nonumber \\\\\n&= f(x_1, x_2, \\ldots, x_N | \\theta) \\nonumber\n\\end{align}\n\\]\nwhich is a function of the parameters \\(\\theta\\)\nFor the examples in this lecture we will additionally assumme that our observations are independent and identically distributed (iid). With this we can simplify the previous expression as\n\\[\n\\begin{align}\n\\mathcal{L}(\\theta) &= f(x_1| \\theta) \\cdot f(x_2| \\theta) \\cdot \\ldots \\cdot f(x_N| \\theta) \\nonumber \\\\\n&= \\prod_{i=1}^N f(x_i| \\theta) \\nonumber\n\\end{align}\n\\]\nThe value of the likelihood itself does not hold much meaning, but it can be used to make comparisons between different values of the parameter vector \\(\\theta\\). The larger the likelihood the better the model, i.e. likelihood maximization allows us to find the best \\(\\theta\\) for our data\nBefore continuing consider the following\n\nLikelihood is not probability\n\n\nThe likelihood of a set of RVs does not integrate (or sum in the discrete case) to unity, i.e. in general the likelihood is not a valid probability density function.\nThe likelihood by itself cannot be interpreted as a probability of \\(\\theta\\), it only tells us how likely is that \\(\\{x_i\\}\\) was generated by the distribution \\(f\\) with parameter \\(\\theta\\)"
  },
  {
    "objectID": "chapters/modeling/part1.html#maximum-likelihood-estimation-mle",
    "href": "chapters/modeling/part1.html#maximum-likelihood-estimation-mle",
    "title": "2  Parametric Modeling",
    "section": "2.3 Maximum Likelihood Estimation (MLE)",
    "text": "2.3 Maximum Likelihood Estimation (MLE)\nIn parametric modeling we are interested in finding \\(\\theta\\) that best fit our observations.\nOne method to do this is MLE:\n\n1 Select a distribution (model) for the observations and formulate the likelihood \\(\\mathcal{L}(\\theta)\\)\n2 Search for \\(\\theta\\) that maximizes \\(\\mathcal{L}(\\theta)\\) given the data, i.e.\n\n\\[\n\\hat \\theta = \\text{arg} \\max_\\theta \\mathcal{L}(\\theta),\n\\]\nwhere the point estimate \\(\\hat \\theta\\) is called the maximum likelihood estimator of \\(\\theta\\)\nAfter this we can\n\n3 Determine the confidence region of \\(\\hat \\theta\\) either analytically or numerically (bootstrap, cross-validation, etc)\n4 Make conclusions about your model (hypothesis test)\n\n\nA wrong assumption in step 1 can ruin your inference. Evaluate how appropriate your model is, compare with other models and suggest incremental improvements\n\n\n2.3.1 Example: MLE for the mean of a Gaussian distribution\nLet us consider a set of N measurements \\(\\{x_i\\}_{i=1,\\ldots, N}\\) obtained from a sensor and that all were obtained under the same conditions\nThe sensor used to measure the data has an error that follows a Gaussian distribution with known variance \\(\\sigma^2\\)\nIf the conditions are the same then the measurements can be viewed as noisy realizations of the true value \\(\\mu\\)\n\\[\nx_i = \\mu + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n\\]\nand we can write the distribution of \\(x_i\\) as\n\\[\nf(x_i) = \\mathcal{N}(x_i |\\mu,\\sigma^2) \\quad \\forall i\n\\]\nFinally, the likelihood of \\(\\mu\\) given the measurements and the variance \\(\\sigma^2\\) is\n\\[\n\\mathcal{L}(\\mu) = f(\\{x_i\\}| \\mu, \\sigma^2) = \\prod_{i=1}^N f(x_i| \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\prod_{i=1}^N  \\exp  \\left( -\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right)\n\\]\nWe can find \\(\\mu\\) by maximizing the likelihood given \\(\\{x_i\\}\\)\n\nIn many cases it is more practical to find the maximum of the logarithm of the likelihood (e.g. distributions from the exponential family). Logarithm is a monotonic function and its maximum is the same as its argument.\n\nIn this case the log likelihood is\n\\[\n\\begin{align}\n\\log \\mathcal{L} (\\mu) &= \\log \\prod_{i=1}^N f(x_i|\\mu, \\sigma^2) \\nonumber \\\\\n&= \\sum_{i=1}^N \\log f(x_i|\\mu, \\sigma^2) \\nonumber \\\\\n&= - \\frac{1}{2} \\sum_{i=1}^N \\log 2\\pi\\sigma^2 - \\frac{1}{2} \\sum_{i=1}^N  \\frac{(x_i-\\mu)^2}{\\sigma^2}  \\nonumber  \\\\\n&=  - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2 \\nonumber\n\\end{align}\n\\]\nWe maximize by making the derivative of the log likelihood equal to zero\n\\[\n\\frac{d  \\log \\mathcal{L} (\\mu)}{d\\mu} =  \\frac{1}{\\sigma^{2}}  \\sum_{i=1}^N (x_i-\\mu) =0\n\\]\nFinally the MLE of \\(\\mu\\) is\n\\[\n\\hat \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i, \\quad \\sigma >0,\n\\]\nThe sample mean is the MLE estimator of the mean for a Gaussian likelihood\n\n\n2.3.2 Example: MLE for the variance of a Gaussian distribution\nLet’s say now that we don’t know the variance of the noise of the sensor\nThe MLE estimator of the variance can be obtained using the same procedure:\n\\[\n\\log \\mathcal{L} (\\mu, \\sigma^2) =  - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2\n\\]\n\\[\n\\frac{d  \\log \\mathcal{L} (\\mu, \\sigma^2)}{d\\sigma^2} =  - \\frac{N}{2} \\frac{1}{\\sigma^2} + \\frac{1}{2\\sigma^{4}}\\sum_{i=1}^N (x_i-\\mu)^2 =0\n\\]\n\\[\n\\hat \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^N (x_i- \\hat\\mu)^2\n\\]\n\nIf the true mean is not known then this is a biased estimator of the true variance. MLE does not guarantee unbiased estimators\n\nThe following code example shows how the value of the MLEs of these parameters evolve as more data is observed\n\n\nCode\nimport holoviews as hv\nhv.extension('bokeh')\nimport numpy as np\nimport scipy.signal\n\n# data from the sensor\nnp.random.seed(1234)\nx = 80 + np.random.randn(10000)\n#x = 80 + 2*np.random.rand(1000)  # What happens if the data is not normal\n\n# Computing the MLE\nNs = np.round(np.logspace(0, 4, num=16)).astype('int')\nhat_mu = np.array([np.mean(x[:N]) for N in Ns])\nhat_s2 = np.array([np.mean((x[:N]-mu)**2) for mu, N in zip(hat_mu, Ns)])\n\n\n\nmu_plot = hv.Curve((Ns, hat_mu), 'Number of samples', 'hat mu')*hv.HLine(80).opts(color='k')\ns2_plot = hv.Curve((Ns, hat_s2), 'Number of samples', 'hat s2')*hv.HLine(1).opts(color='k')\n(mu_plot + s2_plot).opts(hv.opts.Curve(logx=True, width=350), hv.opts.HLine(line_dash='dashed'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n/home/phuijse/.conda/envs/quarto/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n\nUnable to display output for mime type(s): \n\n\n\n  \n\n\n\n\n\n\n2.3.3 A note on biased and unbiased estimators\nFor a parameter \\(\\theta\\) and an estimator \\(\\hat \\theta\\), if\n\\[\n\\mathbb{E}[\\hat \\theta] = \\theta,\n\\]\nthen \\(\\hat \\theta\\) is an unbiased estimator of \\(\\theta\\)\nIs the MLE of \\(\\mu\\) unbiased?\n\\[\n\\begin{align}\n\\mathbb{E}[\\hat \\mu] &= \\mathbb{E} \\left[ \\frac{1}{N} \\sum_{i=1}^N x_i \\right]  \\nonumber \\\\\n&= \\frac{1}{N} \\sum_{i=1}^N \\mathbb{E}[x_i] = \\frac{1}{N} \\sum_{i=1}^N \\mu = \\mu  \\nonumber\n\\end{align}\n\\]\n\nThe answer is YES\n\nIs the MLE of \\(\\sigma^2\\) unbiased?\nFirst lets expand the expression of the MLE of the variance\n\\[\n\\begin{align}\n\\hat \\sigma^2 &= \\frac{1}{N} \\sum_{i=1}^N \\left(x_i- \\frac{1}{N}\\sum_{j=1}^N x_j \\right)^2 \\nonumber \\\\\n&= \\frac{1}{N} \\sum_{i=1}^N x_i^2 - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N x_i  x_j \\nonumber \\\\\n&= \\frac{1}{N} \\sum_{i=1}^N x_i^2 - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j\\neq i} x_i x_j - \\frac{1}{N^2} \\sum_{i=1}^N x_i^2 \\nonumber  \\\\\n&= \\frac{N-1}{N^2} \\sum_{i=1}^N x_i^2 - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j \\neq i} x_i x_j  \\nonumber\n\\end{align}\n\\]\nThen applying the expected value operator we get\n\\[\n\\begin{align}\n\\mathbb{E}[\\hat \\sigma^2] &= \\frac{N-1}{N^2} \\sum_{i=1}^N \\mathbb{E} [x_i^2] - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j \\neq i} \\mathbb{E} [x_i] \\mathbb{E} [x_j] \\nonumber  \\\\\n&= \\frac{N-1}{N} (\\sigma^2 + \\mu^2) - \\frac{N-1}{N} \\mu^2 \\nonumber \\\\\n&= \\frac{N-1}{N} \\sigma^2 \\neq \\sigma^2  \\nonumber\n\\end{align}\n\\]\n\nThe answer is NO\n\nCan we correct for the bias?\n\nIn this case, YES!\n\nIf we multiply it by \\(\\frac{N}{N-1}\\) we obtain the well known unbiased estimator of the variance\n\\[\n\\hat \\sigma_{u}^2 = \\frac{N}{N-1} \\hat \\sigma^2 = \\frac{1}{N-1} \\sum_{i=1}^N (x_i- \\hat\\mu)^2\n\\]\n\n\n2.3.4 Example: MLE of a Gaussian mixture\nLet’s imagine that our iid data come from a mixture of Gaussians with K components\n\\[\nf(x_i|\\pi,\\mu,\\sigma^2) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k, \\sigma_k^2),\n\\]\nwhere \\(\\sum_{k=1}^K \\pi_k = 1\\) and \\(\\pi_k \\in [0, 1] ~~ \\forall k\\)\nWe can write the log likelihood as\n\\[\n\\log \\mathcal{L}(\\pi,\\mu,\\sigma^2) = \\sum_{i=1}^N \\log \\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k, \\sigma_k^2)\n\\]\n\nIn this case we cannot obtain analytical expressions for the parameters by setting the derivative to zero. We have to resort to iterative methods/optimizers, e.g. gradient descent, expectation maximization #\n\nWe will come back to this in a future class on expectation maximization and Gaussian mixture models"
  },
  {
    "objectID": "chapters/modeling/part1.html#optimality-properties-and-uncertainty-of-mles",
    "href": "chapters/modeling/part1.html#optimality-properties-and-uncertainty-of-mles",
    "title": "2  Parametric Modeling",
    "section": "2.4 Optimality properties and uncertainty of MLEs",
    "text": "2.4 Optimality properties and uncertainty of MLEs\nAssuming that the data truly comes from the specified model the MLE is\nConsistent: The estimate converge to the true parameter as data points increase\n\\[\n\\lim_{N\\to \\infty} \\hat \\theta = \\theta\n\\]\nAsymptotically normal: The distribution of the estimate approaches a normal centered at the true parameter.\n\\[\n\\lim_{N\\to \\infty} p(\\hat \\theta) = \\mathcal{N}(\\hat \\theta | \\theta, \\sigma_\\theta^2),\n\\]\nwhich is a consequence of the central limit theorem\nFor i.i.d. \\(\\{X_i\\}, i=1,\\ldots,N\\) with \\(\\mathbb{E}[X] < \\infty\\) and \\(\\text{Var}[X] < \\infty\\) then\n\\[\n\\lim_{N\\to\\infty} \\sqrt{N} (\\bar X - \\mathbb{E}[X]) = \\mathcal{N}(0, \\sigma^2)\n\\]\nBecause MLE have asymptotically normal distributions the log likelihood ratio have asymptotically a chi-square distributions (more about this later)\nMinimum variance: The estimate achieve the theoretical minimal variance given by the Cramer-Rao bound. This bound is the inverse of the expected Fisher information, i.e the second derivative of \\(- \\log L\\) with respect to \\(\\theta\\)\n\\[\n\\sigma_{nm}^2 =  \\left (- \\frac{d^2 \\log \\mathcal{L} (\\theta)}{d\\theta_n \\theta_m} \\bigg\\rvert_{\\theta = \\hat\\theta}\\right)^{-1}\n\\]\n\n\n\\(\\sigma_{nm}^2\\) is the minimum variance achieved by an unbiased estimator.\n\\(\\sigma_{nn}^2\\) gives the marginal error bars\nIf \\(\\sigma_{nm} \\neq 0 ~~ n\\neq m\\), then errors are correlated, i.e some combinations of parameters might be better determined than others\n\n\n\n2.4.1 Example: Cramer-Rao bound for the MLE of \\(\\mu\\)\nConsidering a Gaussian likelihood from the previous example\n\\[\n\\log \\mathcal{L} (\\mu, \\sigma^2) =  - \\frac{N}{2} \\log 2\\pi\\sigma^2 - \\frac{1}{2\\sigma^{2}}   \\sum_{i=1}^N (x_i-\\mu)^2\n\\]\nWhat is the uncertainty of the MLE of \\(\\mu\\)? In this case the Cramer-rao bound\n\\[\n\\begin{align}\n\\sigma_{\\hat\\mu}^2  &= \\left (- \\frac{d^2 \\log \\mathcal{L}(\\mu, \\sigma^2)}{d\\mu^2} \\bigg\\rvert_{\\mu=\\hat\\mu}\\right)^{-1}  \\nonumber \\\\\n&=  \\left (- \\frac{1}{\\sigma^2} \\frac{d}{d\\mu}  \\sum_{i=1}^N (x-\\mu) \\bigg\\rvert_{\\mu=\\hat\\mu}\\right)^{-1}  \\nonumber \\\\\n&=  \\left ( \\frac{N}{\\sigma^2}  \\bigg\\rvert_{\\mu=\\hat\\mu}\\right)^{-1} = \\frac{\\sigma^2}{N}  \\nonumber\n\\end{align}\n\\]\nan expression that is known as the standard error of the mean\nThen we have\n\\[\np(\\hat \\mu) \\to \\mathcal{N}(\\hat \\mu| \\mu, \\sigma^2/N)\n\\]\nLet’s see how the MLE and its uncertainty changes as more data is observed\n\n\nCode\n# Generate data\nnp.random.seed(12345)\nmu_real, s_real = 2.23142, 1.124123\nx = mu_real + s_real*np.random.randn(10000)\n# MLE and its standard error\nhat_mu = np.array([np.mean(x[:n]) for n in range(1, len(x))])\nstandard_error = s_real/np.sqrt(np.arange(1, len(x)))\n\n\nmu_plot = hv.Curve((range(1, len(x)), hat_mu), \n                   kdims='Number of samples', vdims='mu', label='Estimated mu')\nse_plot = hv.Spread((range(1, len(x)), hat_mu, standard_error), \n                    kdims='Number of samples', label='Standard error')\nmu_real_plot = hv.HLine(mu_real).opts(line_dash='dashed', color='k', alpha=0.5)\n(mu_plot*se_plot*mu_real_plot).opts(hv.opts.Curve(logx=True, width=500))\n\n\nUnable to display output for mime type(s):"
  },
  {
    "objectID": "chapters/modeling/part1.html#hypothesis-tests-based-on-the-likelihood",
    "href": "chapters/modeling/part1.html#hypothesis-tests-based-on-the-likelihood",
    "title": "2  Parametric Modeling",
    "section": "2.5 Hypothesis tests based on the likelihood",
    "text": "2.5 Hypothesis tests based on the likelihood\nConsidering the asymptotic distributions shown before we can formulate a hypothesis test for the MLE of \\(\\theta\\)\nWe will present the Wald-test and the Wilks test\n\n2.5.1 Wald-test\nSuppose we wish to test\n\\[\n\\mathcal{H}_0: \\theta = \\theta_0\n\\]\n\\[\n\\mathcal{H}_A: \\theta \\neq \\theta_0\n\\]\nUnder the null we can write\n\\[\nW = \\frac{(\\hat \\theta - \\theta_0)^2}{\\left (- \\frac{d^2 \\log \\mathcal{L} (\\theta)}{d\\theta^2} \\bigg\\rvert_{\\theta = \\hat\\theta}\\right)^{-1}} = (\\hat \\theta - \\theta_0)^2 \\sigma_{\\hat \\theta}^2 \\to \\chi^2_1\n\\]\nThe test statistic have a \\(\\chi^2\\) distribution with one degree of freedom\nIf \\(W\\) is greather than the \\((1-\\alpha)100\\%\\) quantile of \\(\\chi^2_1\\) we reject the null hypothesis\n\n\n2.5.2 Log-likelihood ratio test or Wilks test\nSuppose we wish to test\n\\[\n\\mathcal{H}_0: \\theta = \\theta_0\n\\]\n\\[\n\\mathcal{H}_A: \\theta =\\theta_1\n\\]\nWe can write a ratio between likelihoods\n\\[\n\\lambda(\\mathcal{D}) = \\frac{\\mathcal{L}(\\theta_0|\\mathcal{D})}{\\mathcal{L}(\\theta_1|\\mathcal{D})}\n\\]\nAsymptotically, under the null, we have\n\\[\n-2 \\log \\lambda(\\mathcal{D}) \\to \\chi^2_1\n\\]\nIf \\(-2 \\log \\lambda(\\mathcal{D})\\) is greather than the \\((1-\\alpha)100\\%\\) quantile of \\(\\chi^2_1\\) we reject the null"
  },
  {
    "objectID": "chapters/modeling/part1.html#criteria-for-model-comparison",
    "href": "chapters/modeling/part1.html#criteria-for-model-comparison",
    "title": "2  Parametric Modeling",
    "section": "2.6 Criteria for model comparison",
    "text": "2.6 Criteria for model comparison\nHow to compare models with different number of parameters? In general the more number of parameters the better the fit (overfitting). The likelihood does not take into account the complexity (number of parameters) of the model\n\nHow to score models taking into account their complexity?\n\nOne option is to use the Akaike information criterion (AIC). For a model with \\(k\\) parameters and N data points the AIC is\n\\[\n\\text{AIC} = -2 \\log \\mathcal{L}(\\hat \\theta) + 2k + \\frac{2k(k+1)}{N-k-1},\n\\]\nwhich one seeks to minimize. The AIC combines the likelihood (score) of the model and its complexity. The AIC is based on an asumptotic approximation which we will review in the future\nThis is also related to the idea of regularization, which will also be reviewed in future lectures"
  }
]